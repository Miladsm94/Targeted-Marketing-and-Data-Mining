# -*- coding: utf-8 -*-
"""Marketing-final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lzN7zjBqQOiILNIYogDULmNo-gT9tWkm

# Section 1: Business Understanding
## Marketing Campaign
Effective marketing campaigns that consistently produce a favorable return on investment are more important for small business owners due to limited financial resources. Targeted advertisement is the main component of an Effective marketing campaign; however, identifying the target customer group can be challenging. In this project, we will use a publically available dataset for marketing campaigns of a Portuguese banking institution to provide an insight into the potential of data mining in targeted marketing.
### Question 1 : What are the most important factors that affect the clients' decision in accepting the offer?

### Question 2 : Can we justify the extracted insights from the data? 

### Question 3 : Is it feasible to predict the clients' decision prior to making the phone calls?

# Section 2: Data Understanding
## Import Libraries
let's import necessary libraries.
"""

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
drive.mount('/content/drive')

"""## load Data
Import the dataset from the excel file. Report the total number of columns with the categorical and numerical variables.
"""

path = "/content/drive/MyDrive/Book-final.xlsx"

# Read the excel file.
df = pd.read_excel(path)

# Determine the number of rows in the dataset.
num_rows = df.shape[0] 

# Determine the number of columns in the dataset.
num_cols = df.shape[1] 

# Total number of the columns with quantitative variables.
num_var_col = df.select_dtypes(include=["int","float"]).shape[1];

# Total number of the columns with categorical variables.
num_cat_col = df.select_dtypes(include=["object"]).shape[1];

# Print the total number of rows and columns as well as the total number of columns with categorical and numerical variables.

print("The dataset has {} rows and {} columns. There are {} categorical variables and \
{} quantitative variables.".format(num_rows, num_cols, num_cat_col, num_var_col))

"""## Missing Values 

It is necessary to identify the columns with missing values in the dataset. In the following steps based on the number of the missing values, a proper approach will be used to handle the missing values.

"""

# Specify the columns without missing values
no_nulls = set([columns for columns in df.columns if df[columns].notnull().all()])

# Specify the columns with missing values
nulls = set([columns for columns in df.columns if df[columns].isnull().any()])

# Print the column names with missing vlaues.
if nulls:
  print("Columns with missing values: " + str(*nulls)+".")
else:
  print("No missing value to impute.")

"""# Section 3: Data Visualization
## Correlation matrix
A correlation matrix is a simple way to summarize the correlations between features and target value in the dataset.
"""

from sklearn.preprocessing import OrdinalEncoder

# Select the columns with categorical values
df_cat= df.select_dtypes(include='object')

# Implement ordicanl encoder on the categorical columns
OrEn= OrdinalEncoder()
OrEn.fit(df_cat)
df_cat_OrEn=pd.DataFrame(OrEn.transform(df_cat))
df_cat_OrEn.columns=df_cat.columns

# Concatenate the encoded columns with numerical columns
df_modify=pd.concat([df.select_dtypes(include='int'),df_cat_OrEn],axis=1)

# Plot the correlation between attributes and output
sns.set(rc={"figure.figsize":(10, 8)})
sns.set(font_scale=1.4)

sns.heatmap(df_modify.corr(), annot=False, fmt=".5f",linewidths=.5,xticklabels=True, yticklabels=True );

"""The correlation matrix shows that phone call duration has a significant correlation with the customers' response to the offer.
## Visualize the features
1.   Visualize the relationship between "month" and "outcome".
"""

# Group the data by "month" and "outcome"
data_1=df.groupby(["month", "y"]).size().reset_index(name='counts')

# Visualize the relational plot between "month" and "outcome"
sns.set_style("white")
fig_2 = sns.catplot(x="month", y="counts", hue="y",kind="bar",
      height=5, aspect=1.5, palette=["#FF0075","#172774"],legend=False, 
      data=data_1.sort_values("counts", ascending=False));
plt.legend(loc='upper right');

"""The figure shows that the success rate of the marketing campaign is higher in Oct in comparison with May. 
2.   Visualize the relationship between "job" and "outcome".


"""

# Group the data by "job" and "outcome"
data_1=df.groupby(["job", "y"]).size().reset_index(name='counts')

# Visualize the relational plot between "job" and "outcome"
sns.set(font_scale=1.2)
sns.set_style("white")

fig_3= sns.catplot(x="job", y="counts", hue="y",
      kind="bar", height=5, aspect=1.5, palette="bright",legend=False,
      data=data_1.sort_values("counts", ascending=False));
plt.legend(loc='upper right')

# Rotate the x labels by 60 degrees
fig_3.set_xticklabels(rotation=45);

"""The figure shows that the job type of the customers affects their decision in accepting the offer. For instance, retired people are more interested in offers rather than customers who have blue-collar jobs.
3. Visualize the relationship between "day", "duration" ,and "output" .
"""

# Visualize the relation ship between "day", "duration" ,and "output" 
sns.set(rc={"figure.figsize":(10, 7)})
sns.set(font_scale=1.4)
sns.set_style("white")
sns.scatterplot(x="day", y="duration", hue="y", palette=["#172774","#FF0075"], s=90, data=df);

"""The figure shows that regardless of the day of the month, phone call duration affects the customers' decision in accepting the offer.

# Section 4: Modeling and Evaluation
In this step, we aim to build a classification model to predict the outcome of each phone call based on the features. The trained model can be used to shortlist the customers in the feature campaigns.

## Perform One-Hot encoding
"""

df.dtypes

# Perform One-Hot encoding on columns with categorical variables
X_Onehot_cat = pd.get_dummies(df[['job','marital','education','default','housing','loan','contact','month','poutcome']])
X_encoded = pd.concat([X_Onehot_cat,df[['age', 'balance','day','duration','campaign','pdays','previous']]],axis=1)
# Print out few columns of encoded X set
X_encoded.head()
y = df['y'].apply(lambda x:0 if x=='no' else 1)

# Implement SMOTE oversampleing on the dataset
x, y = smote_imbal(X_encoded,y)

# Split into train and test
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size = .30, random_state=42)      

# Print the shape of train and test arrays
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""## SMOTE oversampling
The dataset is imbalanced; so, SMOTE is implemented on the dataset to oversample the minority class.
"""

from imblearn.over_sampling import SMOTE 
def smote_imbal(X,Y):
  ''' 
  The function implements the SMOTE oversampling on the imbalanced dataset.
  
  Input:
  X: feature set
  Y: labels

  Output:
  X_sm: feature set after SMOTE oversampling
  y_sm: labels after SMOTE oversampling
  '''
  smote = SMOTE (sampling_strategy = 'minority')
  X_sm, y_sm = smote.fit_resample(X,Y)
  return X_sm, y_sm

"""## Building classification model"""

# Import the models from Scikit-learn and xgboost libraries
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Define the SVM, KNN, RF, and XGboost models
SVM = SVC(kernel='rbf')
KNN= KNeighborsClassifier(n_neighbors = 5)
RF = RandomForestClassifier(n_estimators=20)
XGboost = XGBClassifier(max_depth=16, learning_rate=0.01)
models=[SVM, KNN,RF,XGboost]

# Define the dictionary to save the cross validation scores for each model
dic={"Model":[],"Max Score":[],"Min Score":[],"Mean Score":[]}
dic["Model"]=["Support Vector Machine","K-Nearest Neighbors","Random Forest",
              "XGboost"]

# Evaluate machine learning models using k-fold cross-validation 
# Save cross valdiation scores
for model in models:
  scores = cross_val_score(model, x, y, cv=5)
  dic["Max Score"].append(scores.max()*100)
  dic["Min Score"].append(scores.min()*100)
  dic["Mean Score"].append(scores.mean()*100)

"""## Print out the cross validation results"""

# Improt tabulate library
from tabulate import tabulate

# Print the cross valdiation scores 
headers=["Model","Max Score","Min Score","Mean Score"]
print("Cross-Validation Results: \n")
print(tabulate(dic,headers))

"""## Conclusion
The result of the analysis shows that "duration" has the highest correlation with the outcome of the marketing campaign which can be justified. The "duration" can be an indicator of the negotiation which means that when the employee spends enough time and convince the customer, the outcome will be positive. Moreover, the results of implementing machine learning algorithms show that the clients' response to the marketing campaign can be predicted with acceptable accuracy. For instance, the response of the customers to the marketing campaign can be predicted with the mean accuracy of 92% using the random forest classifier. Consequently, the proposed approach can be used to shortlist the potential customers before conducting the phone call.
"""